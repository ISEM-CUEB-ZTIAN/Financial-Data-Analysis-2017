#+TITLE: Lecture 2: The ARCH Model
#+AUTHOR: Zheng Tian
#+DATE:
#+STARTUP: beamer
#+OPTIONS: toc:t H:2
#+LATEX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [presentation,10pt]
#+BEAMER_THEME: CambridgeUS
#+BEAMER_COLOR_THEME: beaver
#+COLUMNS: %45ITEM %10BEAMER_env(Env) %10BEAMER_act(Act) %4BEAMER_col(Col) %8BEAMER_opt(Opt)
#+PROPERTY: BEAMER_col_ALL 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.0 :ETC

#+LATEX_HEADER: \usepackage{amsthm}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{amssymb}
#+LATEX_HEADER: \usepackage{mathtools}
#+LATEX_HEADER: \newtheorem{mydef}{Definition}
#+LATEX_HEADER: \newtheorem{mythm}{Theorem}
#+LATEX_HEADER: \newcommand{\dx}{\mathrm{d}}
#+LATEX_HEADER: \newcommand{\var}{\mathrm{Var}}
#+LATEX_HEADER: \newcommand{\cov}{\mathrm{Cov}}
#+LATEX_HEADER: \newcommand{\corr}{\mathrm{corr}}
#+LATEX_HEADER: \newcommand{\pr}{\mathrm{Pr}}
#+LATEX_HEADER: \newcommand{\rarrowd}[1]{\xrightarrow{\text{ \textit #1 }}}
#+LATEX_HEADER: \DeclareMathOperator*{\plim}{plim}
#+LATEX_HEADER: \newcommand{\plimn}{\plim_{n \rightarrow \infty}}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \usepackage{caption}
#+LATEX_HEADER: \usepackage{subcaption}
#+LATEX_HEADER: \def\mathbi#1{\textbf{\em #1}}
#+LATEX_HEADER: \setlength{\parskip}{1em}
#+LATEX_HEADER: \newcommand{\undersetdisp}[2]{\underset{\displaystyle #1}{#2}}

* The Volatility of Asset Returns

** Volatility is measured with the conditional variance

- Volatility here refers to the *conditional variance* of a time series.

- For a return series {$r_t$}, we are now interested in
  \[\sigma^2_t = \var(r_t \mid F_{t-1})\]
  where $F_{t-1}$ is the information set at time $t-1$.

** Characteristics of volatility (1)

1. *There exist volatility clusters.* That is, volatility may be high
   for certain time periods and low for other periods.
   #+CAPTION: Percentage Change in the NYSE U.S.100 stock price index
   #+NAME: fig:nyse-us-100
   #+ATTR_LATEX: :width 0.7\textwidth
   [[file:img/nyse_us100.png]]

** Characteristics of volatility (2)

2. Volatility evolves over time in a continuous manner. That is,
   volatility jumps are rare.
   #+CAPTION: Annualized Growth Rate of Real GDP
   #+NAME: fig:realgdp
   #+ATTR_LATEX: :width 0.7\textwidth
   [[file:img/readgdp.png]]

** Characteristics of volatility (3)

3. Volatility does not diverge to infinity. That is, volatility varies
   within some fixed range. Statistically speaking, this means that
   volatility is often stationary.

\vspace{0.5cm}

4. Volatility seems to react differently to a big price increase or a
   big price drop, referred to as the leverage effect.

* The Structure of a Volatility Model

** The basic idea of building a volatility model

Consider the log return series {$r_t$}. The basic idea of a volatility
model is
- {$r_t$} may appear to be either serially uncorrelated or
  serially correlated with a minor order.
- However, {$r_t$} is a dependent series and the dependence arises
  from its conditional variance.

#+CAPTION: Time plot of monthly log returns of Intel stock from January 1973 to December 2008
#+NAME: fig:intel-return
#+ATTR_LATEX: :width 0.7\textwidth :height 0.4\textheight
[[file:img/intel.png]]

** The sample ACF of {$r_t$} and {$r^2_t$}

#+CAPTION: Sample ACF and PACF of various functions of monthly log stock returns of Intel Corporation from January 1973 to December 2008: (a) ACF of the log returns, (b) ACF of the squared log returns, (c) ACF of the absolute log returns, and (d) PACF of the squared log returns.
#+NAME: fig:acf-intel-return
#+ATTR_LATEX: :width 0.9\textwidth :height 0.5\textheight
[[file:img/acf_intel.png]]

** Decompose $r_t$ into the mean and variance equations

To capture the dependence in a time series through its second moment
but not the mean, we model the mean process and the variance process
separately.

For a return series {$r_t$}, we can model it as
\begin{equation}
\label{eq:mean-plus-var}
r_t = \mu_t + a_t
\end{equation}
where $\mu_t$ represents the conditional mean and $a_t$ is
modeled to capture the conditional variance.

** The mean equation

\begin{align}
&\mu_t = E(r_t \mid F_{t-1}) = \sum_{i=1}^p \phi_i y_{t-i} - \sum_{i=1}^q \theta_i a_{t-i} \label{eq:mean-equation} \\
&y_t = r_t - \phi_0 - \sum_{i=1}^k \beta_i x_{it} \nonumber
\end{align}
$F_{t-1}$ is the information set at time $t-1$.

\vspace{0.5cm}

If you combine these two equations, and let $\mu_t = r_t - a_t$, you
will find that it is just an ARMA$(p, q)$ model with additional
regressors $x_{it}$.

** The variance equation

Denote the conditional variance of $r_t$ with $\sigma^2_t$.
\begin{equation*}
\begin{split}
\sigma^2_t = \var(r_t \mid F_{t-1}) &= E\left( (r_t - E(r_t | F_{t-1}))^2 | F_{t-1} \right) \\
&= E\left( (r_t - \mu_t)^2 \mid F_{t-1} \right) \\
&= \var(a_t \mid F_{t-1})
\end{split}
\end{equation*}

** The variance equation (cont'd)

- If we assume that $E(a_t \mid F_{t-1}) = 0$, we can see that
  $\sigma^2_t = E(a^2_t \mid F_{t-1})$.

- We can use the lagged value of $a^2_t$ to represent the information
  set $F_{t-1}$

- The simplest model is a linear model, like the following
  \[ \sigma^2_t = \alpha_0 + \alpha_1 a^2_{t-1} + \cdots + \alpha_m a^2_{t-m} \]

** The procedure of building a volatility model

Building a volatility model for an asset return series consists of
four steps:

1. Specify a mean equation by testing for serial dependence in the
   data and, if necessary, building an econometric model (e.g., an
   ARMA model) for the return series to remove any linear dependence.

3. Use the squared residuals of the mean equation to test for ARCH
   effects.

4. Specify a volatility model if ARCH effects are statistically
   significant, and perform a joint estimation of the mean and
   volatility equations.

5. Check the fitted model carefully and refine it if necessary.

** Testing for the presence of ARCH effect

*** The Ljung-Box test for the series of $a^2_t$

Upon obtaining the residuals from the estimation
of an adequate mean equation, we can use the squared residuals
{$\hat{a}_t^2$} to test the existence of autocorrelation.
- The Ljung-Box test is used to test the null hypothesis
  $H_0: \rho_1 = \cdots = \rho_m = 0$.
- The $Q(m)$ statistic is
  calculated and compared with the critical value from $\chi^2(m)$
  distribution at the desired significance level.
- The rejection of the
  null hypothesis implies that there is autoregressive conditional
  heteroskedastic (ARCH) effect.

** The LM test

*** An auxiliary regression
We estimate a AR$(m)$ model regarding {$\hat{a}^2_t$}, that is,
\[ \hat{a}^2_t = \alpha_0 + \alpha_1 \hat{a}_{t-1}^2 + \cdots +
\alpha_m \hat{a}^2_{t-m} + e_t \]

*** The LM test
With this model, we test the joint hypothesis
\[H_0: \alpha_1 = \cdots = \alpha_m = 0 \]
- The LM statistic is $NR^2$ where $N$ is the sample size of this
  regression and $R^2$ is the coefficient of the determination of this
  regression.
- Given the null hypothesis is true, this statistic follows
  a $\chi^2(m)$ distribution.

** The LM test (cont'd)

Alternatively, we can use F statistic to test the joint
hypothesis.
- Let $SSR_0 = \sum_{t=m+1}^{T} (\hat{a}^2_{t} -
  \bar{\omega})^2$, where $\bar{\omega} = (1/T) \sum_{t=1}^T
  \hat{a}^2_t$.
- Let $SSR_1 = \sum_{t=m+1}^T \hat{e}^2_t$ where $\hat{e}_t$ is the
  residuals from the regression.
- The F statistic is
  \[F = \frac{(SSR_0 - SSR_1)/m}{SSR_1/(T-2m-1)} \sim F(m, T-2m-1)\]
- Rejecting the null hypothesis motivates us to model the possible
  ARCH effect.

** An example

Go back to Figure [[fig:acf-intel-return]]. Since the return series is
already stationary, we directly test the squared return series to
check the ARCH effect.

- In the LM test of the ARCH effect, $F = 53.62$ and the p value is
  close to zero.
- The Ljungâ€“Box statistics of the $a^2_t$ series also
  shows strong ARCH effects with $Q(12) = 89.85$, the p value of which is
  close to zero.
- Therefore, we can confirm that the return series of
  Intel stock has an ARCH effect, and next we need to model such an
  effect.

* The ARCH Model

** The basic idea of an ARCH model

Consider a series of shocks {$a_t$} in a return series {$r_t$}. The
basic idea of an Autoregressive Conditional Heteroskedasticity (ARCH)
model is

1. the shock $a_t$ of the return series is serially uncorrelated but
   dependent;

2. the dependence of $a_t$ can be modeled through an autoregressive
   process of $a^2_t$.

** The ARCH(m) model

An ARCH(m) model takes the following form
\begin{equation}
\label{eq:archm}
a_t = \sigma_t \epsilon_t,\; \sigma^2_t = \alpha_0 + \alpha_1 a^2_{t-1} + \cdots + \alpha_m a^2_{t-m}
\end{equation}
where $\epsilon_t \sim i.i.d.(0, 1)$, $\alpha_0 > 0$ and $\alpha_i
\geq 0$ for $i=1, \ldots, m$.

- The assumption of $\var(\epsilon_t)=1$ is to make the analysis
  regarding the properties of the ARCH(m) model easy;
- The assumption of $\alpha_0 > 0$ and $\alpha_i \geq 0$ is to ensure
  the conditional variance of $a_t$ is positive.
- $\alpha_1, \ldots, \alpha_m$ should also satisfy some regularity
  conditions to ensure the unconditional variance of $a_t$ is finite.

** The Properties of an ARCH Model

- Let's take an ARCH(1) model as an example to discuss the properties of
  ARCH model.
- The goal is to see how such a model can capture the basic idea mentioned
  above and the stylized fact that highly volatile periods tend to be followed by
  high volatility periods.

Assume an ARCH(1) model as follows
\begin{equation}
\label{eq:arch1}
a_t = \sigma_t \epsilon_t,\; \sigma^2_t = \alpha_0 + \alpha_1 a^2_{t-1},\; \epsilon_t \sim i.i.d.(0, 1)
\end{equation}
where $a_0 > 0$ and $a_1 \geq 0$.

** The unconditional mean and variance of $a_t$

*** The unconditional mean
\begin{equation*}
E(a_t) & = E(\sigma_t \epsilon_t) = E(\sigma_t) E(\epsilon_t) = 0
\end{equation*}
The second equality is ensured because $\sigma_t$ and $\epsilon_t$ are
independent, and the third equality comes from the assumption of
$E(\epsilon_t)=0$.

*** The unconditional variance
\begin{equation*}
\begin{split}
\var(a_t) &= E(a^2_t) = E(\sigma^2_t \epsilon^2_t) \\
&= E(\alpha_0 + \alpha_1 a^2_{t-1}) \cdot 1 = \alpha_0 + \alpha_1\var(a_{t-1})
\end{split}
\end{equation*}

Assuming the unconditional mean of $a_t$ is a constant(why?), we can
have

\[\var(a_t) = \frac{\alpha_0}{1-\alpha_1} \]

Since the variance should be positive and finite, we must have $0 \leq
\alpha_1 < 1$.

# The reason that we need to assume the unconditional mean of $a_t$ to
# be constant and finite is that we assume the return series {$r_t$}
# itself is constant. Keep in mind that a complete ARCH model also
# includes a mean equation for the return series, say, an ARMA model.

** The unconditional covariance of $a_t$

Since $\epsilon_t$ and $\epsilon_{t-i}$ for $i \neq 0$ are independent,

\begin{equation*}
\begin{split}
\cov(a_t, a_{t-i}) &= E(a_t a_{t-i}) = E(\sigma_t \epsilon_t \sigma_{t-i} \epsilon_{t-i}) \\
&= E(\sigma_t \sigma_{t-i}) E(\epsilon_t \epsilon_{t-i}) = 0
\end{split}
\end{equation*}

- What we get now?
  - $a_t$ has constant unconditional mean and variance,
  - $a_t$ is serially uncorrelated.

** The kurtosis of $a_t$

- Assume that $\epsilon \sim N(0, 1)$, implying that $E(\epsilon^4_t) =
  3$. Thus, we have
  \begin{equation*}
  \begin{split}
  E(a^4_t) &= E(\sigma^4_t \epsilon_t^4) = E(\sigma^4_t) E(\epsilon^4_t) = 3 E(\sigma^4_t) \\
  &= 3\left(\alpha^2_0 + 2\alpha_0\alpha_1 E(a^2_{t-1}) + \alpha^2_1 E(a^4_{t-1}) \right)
  \end{split}
  \end{equation*}

- Assume that $a_t$ is fourth-order stationary so that we can define
  $m_4 = E(a^4_t) = E(a^4_{t-1})$. Then, using the fact that $E(a^2_t) =
  \alpha_0 /(1-\alpha_1)$, we can solve $m_4$ from the
  above equation.
  \[m_4 = \frac{3\alpha^2_0(1+\alpha_1)}{(1-\alpha_1)(1-3\alpha^2_1)}
  \]

** The kurtosis of $a_t$ (cont'd)

This result regarding $m_4$ has two important implications:
1) Since the fourth moment of $a_t$ is positive, we see that $\alpha_1$ must
   also satisfy the condition $1-3\alpha_1^2 > 0$, that is, $0 \leq
   \alpha^2_1 < \frac{1}{3}$.
2) The kurtosis of $a_t$ is
   \[\text{kurtosis} = \frac{E(a^4_t)}{E(a^2_t)^2} =
   \frac{3(1-\alpha^2_1)}{1-3\alpha_1^2}  > 3\]

   Thus, the the excess kurtosis of $a_t$ is positive and the tail
   distribution of $a_t$ is heavier than that of a normal
   distribution.

** The conditional mean

- Let's write $E_{t-1}(a_t)$ to represent the conditional mean given the
  information set $F_{t-1}$, i.e., $E(E(a_t \mid F_{t-1}))$.

- Since $\epsilon_t$ is i.i.d, we have $E_{t-1}(\epsilon_t) =
  E(\epsilon_t) = 0$. Thus,

  \begin{equation*}
  \begin{split}
  E_{t-1}(a_t) &= E_{t-1}(\sigma_t \epsilon_t) = E_{t-1}\left((\alpha_0 + \alpha_1 a^2_{t-1})^{1/2} \epsilon_t\right) \\
  &= (\alpha_0 + \alpha_1 a^2_{t-1})^{1/2} E_{t-1}(\epsilon_t) = 0
  \end{split}
  \end{equation*}

** The conditional variance

- The conditional variance of $a_t$ is
  \begin{equation*}
  \begin{split}
  \var_{t-1}(a_t) &= E_{t-1}(a^2_t) = E_{t-1} \left( \sigma^2_t \epsilon_t^2 \right) \\
  &= E_{t-1}\left((\alpha_0 + \alpha_1 a^2_{t-1}) \epsilon^2_t \right) \\
  &= E_{t-1}(\alpha_0 + \alpha_1 a^2_{t-1}) E_{t-1}(\epsilon^2_t) \\
  &= (\alpha_0 + \alpha_1 a^2_{t-1}) E(\epsilon^2_t) \\
  &= \alpha_0 + \alpha_1 a^2_{t-1} = \sigma^2_t
  \end{split}
  \end{equation*}

- How does the conditional variance capture the stylized fact?

* Maximum Likelihood Estimation

** The definition of the likelihood function

The likelihood function is the joint density function $f(\mathbf{y} |
\boldsymbol{\theta})$ when we consider it as a function of the
parameters $\boldsymbol{\theta}$ given a set of data $\mathbf{y}$.

- $\mathbf{y} = (y_1, \ldots, y_T)$ represents the observations,
  which are assumed to be identically independently distributed.
- $\boldsymbol{\theta} = (\theta_1, \ldots, \theta_k)$ represents all
  parameters in the model (i.e., data generating process) that
  generates $\mathbf{y}$.
- For each observation $y_t$, its (marginal) PDF is $f(y_t |
  \boldsymbol{\theta})$. 

** The likelihood function when $y_t$ is independent

*** The likelihood function
\begin{equation}
\label{eq:general-likelihood}
L(\boldsymbol{\theta} | \mathbf{y}) = \prod_{t=1}^T f(y_t | \boldsymbol{\theta})
\end{equation}

*** The log-likelihood function
\begin{equation}
\label{eq:general-logL}
\ell(\boldsymbol{\theta} | \mathbf{y}) = \sum_{t=1}^T \ell_t(\boldsymbol{\theta} | y_t)
\end{equation}
where $\ell_t(\boldsymbol{\theta} | y_t) = \ln\left(f(y_t |
\boldsymbol{\theta})\right)$ is the *contribution* to the
loglikelihood function.

** The joint density when $y_t$ is dependent

If $y_1, y_2, \ldots, y_T$ are dependent, their joint density can be
written as
\[ f(y_1, y_2, \ldots, y_T) = f(y_1) f(y_2 | y_1) \cdots f(y_T | y_1,
\ldots, y_{T-1}) \]
or conveniently denoted as
\[f(\mathbf{y}^T) = \prod_{t=1}^T f(y_t | \mathbf{y}^{t-1}) \]

** The likelihood and log-likelihood functions when $y_t$ is dependent

When $\mathbf{y}$ is also dependent on $\boldsymbol{\theta}$, the
likelihood function is then
\begin{equation}
\label{eq:depend-likelihood}
L(\boldsymbol{\theta} | \mathbf{y}^T) = \prod_{t=1}^T f(y_t | \mathbf{y}^{t-1}, \boldsymbol{\theta})
\end{equation}
And the log-likelihood function is
\begin{equation}
\label{eq:depend-logL}
\ell(\boldsymbol{\theta} | \mathbf{y}^T) = \sum_{t=1}^T \ell_t(\boldsymbol{\theta} | \mathbf{y}^{t})
\end{equation}

** The maximum likelihood (ML) estimator

- The ML estimator maximizes the log-likelihood function over the
  parameter space in which $\boldsymbol{\theta}$ lies in.

  \begin{equation}
  \operatorname*{max}_{\{\theta \in \Theta\}}\: \ell(\boldsymbol{\theta} | \mathbf{y}^T) = \sum_{t=1}^T \ell_t(\boldsymbol{\theta} | \mathbf{y}^t)
  \end{equation}

- The ML estimator is usually obtained by computational methods, like
  the Newton or quasi-Newton method.

* Estimation of an ARCH(m) model

** The assumption of the distribution of $\epsilon_t$

Consider an ARCH(m) model

\begin{equation*}
a_t = \sigma_t \epsilon_t,\; \sigma^2_t = \alpha_0 + \alpha_1 a^2_{t-1} + \cdots + \alpha_m a^2_{t-m}
\end{equation*}

- Assume that $\epsilon_t \sim N(0, 1)$ and $\epsilon_t$ is i.i.d.

- How can we get the likelihood function of an ARCH(m) model based on
  this assumption?

** The distribution of $a_t$

*** The conditional distribution of $a_t$
\[ \epsilon_t \sim N(0, 1) \Rightarrow a_t \mid F_{t-1} \sim N(0,
\sigma^2_t) \] 
where $\sigma^2_t$ is given by the ARCH(m) model. The
conditional PDF of each $a_t$ for $t=1, \ldots, T$ is

*** The conditional PDF of $a_t$
\[
f(a_t | F_{t-1}) = \frac{1}{\sqrt{2 \pi \sigma^2_t}} \exp
\left(-\frac{a^2_t}{2\sigma^2_t}\right)
\] 
where $F_{t-1}$ is the information set represented by $(a_1, \ldots,
a_{t-1})$.

** The joint density for $a_1, \ldots, a_T$ given the parameter vector $\mathbf{\alpha}$

- For convenience, we suppress $\boldsymbol{\alpha} = (\alpha_0,
  \alpha_1, \ldots, \alpha_m)$ in the
  conditional joint density $f(a_1, \ldots, a_T \mid
  \boldsymbol{\alpha})$.

- Since $a_1, \ldots, a_T$ are not independent, we have the joint
  density as
  \begin{align}
  f(a_1, a_2, \ldots, a_T) &= f(a_T | F_{T-1}) f(a_{T-1} | F_{T-2}) \cdots f(a_{m+1} | F_m) f(a_1, \ldots, a_m) \nonumber \\
  &= \prod_{t=m+1}^T \frac{1}{\sqrt{2 \pi \sigma^2_t}} \exp \left(-\frac{a^2_t}{2\sigma^2_t}\right) \times f(a_1, \ldots, a_m)
  \end{align}

** The conditional likelihood function

Dropping $f(a_1, \ldots, a_m)$ because the exact form of it is often
complicated, we get the conditional likelihood function
\begin{equation}
\label{eq:arch-likelihood}
L(\boldsymbol{\alpha} |a_1, \ldots, a_T) = \prod_{t=m+1}^T \frac{1}{\sqrt{2 \pi \sigma^2_t}} \exp \left(-\frac{a^2_t}{2\sigma^2_t}\right)
\end{equation}

- $\boldsymbol{\alpha}$ enters the likelihood function through 
  \[\sigma^2_t = \alpha_0 + \alpha_1 a^2_{t-1} + \cdots + \alpha_m
  a^2_{t-m} \]

- Maximizing the likelihood function results in the maximum likelihood
  estimator under the normality assumption. 

** The conditional log-likelihood function

Taking logarithm of the likelihood function yields the log-likelihood
function
\begin{equation}
\label{eq:arch-logL}
\ell(\boldsymbol{\alpha} | a_1, \ldots, a_T) = 
\sum_{t=m+1}^T \left[ -\frac{1}{2} \ln(2\pi) - \frac{1}{2} \ln(\sigma^2_t) - \frac{1}{2} \frac{a^2_t}{\sigma^2_t}  \right]
\end{equation}

The log-likelihood function can be simplified as 
\[
\ell(\boldsymbol{\alpha} | a_1, \ldots, a_T) = -\frac{1}{2}
\sum_{t=m+1}^T \left[\ln(\sigma^2_t) + \frac{a^2_t}{\sigma^2_t}  \right]
\]

** Order determination

- Before estimating an ARCH(m) model, we need to determine the order
  $m$.

  \vspace{0.2cm}

- The basic idea is that we treat an ARCH(m) model as an AR process
  of {$a^2_t$}, and apply the partial autocorrelation function (PACF) to
  determine $m$.

** Why using the PACF?

We justify the use of the PACF of {$a^2_t$} to determine $m$ through
two perspectives.

\vspace{0.2cm}

1. We can consider $a^2_t$ as an unbiased estimator of $\sigma^2_t$
   given the sample data because $E_{t-1}(a^2_t) =
   \sigma^2_t$. Therefore, we use $a^2_t$ as an approximate to
   $\sigma^2_t$.

   \vspace{0.2cm}

2. We can define $\eta_t = a^2_t - \sigma^2_t$. It can be shown that
   - $E(\eta_t) = 0$ and $E(\eta_t \eta_{t-s})=0$ for $s > 0$.
   - But $\eta_t$ is not i.i.d. because $a^2_t$ is dependent.

   \vspace{0.2cm}

   So an ARCH(m) model is essentially an AR(m) model, except
   that $\eta_t$ is not i.i.d. That is,
   \[ a^2_t = \alpha_0 + \alpha_1 a^2_{t-1} + \cdots + \alpha_m
   a^2_{t-m} + \eta_t \]

** Alternative distribution assumptions of $\epsilon_t$

- $\epsilon_t$ has a Student-t distribution: capture the heavy-tail
  of volatility.

- $\epsilon_t$ has a skew-Student-t distribution: capture the skewness
  of asset return.

- $\epsilon_t$ has a general error distribution: enclose the normal
  distribution and heavy-tail distributions. 

* Model checking and forecasting

** Model checking

*** The standardized residuals

Compute the standardized residuals
\[ \tilde{a}_t = \frac{\hat{a}_t}{\hat{\sigma}_t} \]
which should mimic the behavior of $\epsilon_t$. 

*** Check the mean equation

- Use the Ljung-Box statistic for {$\tilde{a}_t$}.

*** Check the volatility equation

- Use the Ljung-Box statistic for {$\tilde{a}_t$}.
- Use the QQ plot or the Shapiro-Wilk test for normality assumption.

** Forecasting

*** 1-step-ahead forecast

\[\sigma^2_h(1) = \alpha_0 + \alpha_1 a^2_h + \cdots + \alpha_m
a^2_{h+1-m} \]

*** 2-step-ahead forecast

\[ \sigma^2_h(2) = \alpha_0 + \alpha_1 \sigma^2_h(1) + \alpha_2
a^2_h + \cdots + \alpha_m a^2_{h+2-m}  \]

*** \ell-step-ahead forecast

\[ \sigma^2_h(\ell) = \alpha_0 + \sum_{i=1}^m \alpha_i
\sigma^2_h(\ell-i) \]

* Applications of ARCH Models

** An ARCH model for the monthly log returns of Intel stock

*** The proposed model

\[ r_t = \mu + \alpha_t, a_t = \sigma_t \epsilon_t, \sigma^2_t =
\alpha_0 + \alpha_1 a^2_{t-1} + \alpha_2 a^2_{t-2} + \alpha_3 a^2_{t-3} \]

*** The estimated model

\[ r_t = \underset{(0.0057)}{0.0122} + a_t, 
\sigma^2_t = \underset{(0.0010)}{0.0106} + 
             \underset{(0.0757)}{0.2131}a^2_{t-1} +
             \underset{(0.0480)}{0.0770}a^2_{t-2} + 
             \underset{(0.0688)}{0.0599}a^2_{t-3}
\]

Since $\alpha_2$ and $\alpha_3$ are statistically insignificant, we
drop the last two terms and re-estimate the model.
\[
r_t = \underset{(0.0053)}{0.0126} + a_t, 
\sigma^2_t = \underset{(0.0010)}{0.0111} + \underset{(0.0761)}{0.3560}a^2_{t-1}
\]

** Model checking

*** The Ljung-Box test
- $Q(10)$ for {$\tilde{a}_t$} is 12.64, p-value = 0.24 \Rightarrow no
  autocorrelation. 
- $Q(10)$ for {$\tilde{a}^2_t$} is 14.75, p-value = 0.14 \Rightarrow
  no autocorrelation. 

*** The sample ACF

#+ATTR_LATEX: :width 0.9\textwidth :height 0.6\textheight
[[file:img/intel_arch.png]]

