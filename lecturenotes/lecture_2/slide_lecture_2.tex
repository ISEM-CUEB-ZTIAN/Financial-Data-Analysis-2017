% Created 2017-05-01 Mon 20:44
% Intended LaTeX compiler: pdflatex
\documentclass[presentation,10pt]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\newtheorem{mydef}{Definition}
\newtheorem{mythm}{Theorem}
\newcommand{\dx}{\mathrm{d}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\corr}{\mathrm{corr}}
\newcommand{\pr}{\mathrm{Pr}}
\newcommand{\rarrowd}[1]{\xrightarrow{\text{ \textit #1 }}}
\DeclareMathOperator*{\plim}{plim}
\newcommand{\plimn}{\plim_{n \rightarrow \infty}}
\usepackage{booktabs}
\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}
\def\mathbi#1{\textbf{\em #1}}
\setlength{\parskip}{1em}
\usetheme{CambridgeUS}
\usecolortheme{beaver}
\author{Zheng Tian}
\date{}
\title{Lecture 2: The ARCH Model}
\hypersetup{
 pdfauthor={Zheng Tian},
 pdftitle={Lecture 2: The ARCH Model},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 25.1.1 (Org mode 9.0.3)}, 
 pdflang={English}}
\begin{document}

\maketitle
\begin{frame}{Outline}
\setcounter{tocdepth}{1}
\tableofcontents
\end{frame}


\section{The Volatility of Asset Returns}
\label{sec:orgeef5cc0}

\begin{frame}[label={sec:org0745415}]{Volatility is measured with the conditional variance}
\begin{itemize}
\item Volatility here refers to the \alert{conditional variance} of a time series.

\item For a return series \{\(r_t\)\}, we are now interested in 
\[\sigma^2_t = \var(r_t \mid F_{t-1})\]
where \(F_{t-1}\) is the information set at time \(t-1\).
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org114e90d}]{Characteristics of volatility (1)}
\begin{enumerate}
\item \alert{There exist volatility clusters.} That is, volatility may be high
for certain time periods and low for other periods. 
\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{img/nyse_us100.png}
\caption{\label{fig:org2968b36}
Percentage Change in the NYSE U.S.100 stock price index}
\end{figure}
\end{enumerate}
\end{frame}

\begin{frame}[label={sec:org143da87}]{Characteristics of volatility (2)}
\begin{enumerate}
\item Volatility evolves over time in a continuous manner. That is,
volatility jumps are rare.
\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{img/readgdp.png}
\caption{\label{fig:org6becf0d}
Annualized Growth Rate of Real GDP}
\end{figure}
\end{enumerate}
\end{frame}

\begin{frame}[label={sec:org055e6fd}]{Characteristics of volatility (3)}
\begin{enumerate}
\item Volatility does not diverge to infinity. That is, volatility varies
within some fixed range. Statistically speaking, this means that
volatility is often stationary.
\end{enumerate}

\vspace{0.5cm}

\begin{enumerate}
\item Volatility seems to react differently to a big price increase or a
big price drop, referred to as the leverage effect.
\end{enumerate}
\end{frame}

\section{The Structure of a Volatility Model}
\label{sec:org5b6b34d}

\begin{frame}[label={sec:org0de5a0d}]{The basic idea of building a volatility model}
Consider the log return series \{\(r_t\)\}. The basic idea of a volatility
model is 
\begin{itemize}
\item \{\(r_t\)\} may appear to be either serially uncorrelated or
serially correlated with a minor order.
\item However, \{\(r_t\)\} is a dependent series and the dependence arises
from its conditional variance.
\end{itemize}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth,height=0.4\textheight]{img/intel.png}
\caption{\label{fig:org3b1a877}
Time plot of monthly log returns of Intel stock from January 1973 to December 2008}
\end{figure}
\end{frame}

\begin{frame}[label={sec:org34a0aff}]{The sample ACF of \{\(r_t\)\} and \{\(r^2_t\)\}}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth,height=0.5\textheight]{img/acf_intel.png}
\caption{\label{fig:org96628fd}
Sample ACF and PACF of various functions of monthly log stock returns of Intel Corporation from January 1973 to December 2008: (a) ACF of the log returns, (b) ACF of the squared log returns, (c) ACF of the absolute log returns, and (d) PACF of the squared log returns.}
\end{figure}
\end{frame}

\begin{frame}[label={sec:org4cf6e4e}]{Decompose \(r_t\) into the mean and variance equations}
To capture the dependence in a time series through its second moment
but not the mean, we model the mean process and the variance process
separately. 

For a return series \{\(r_t\)\}, we can model it as
\begin{equation}
\label{eq:mean-plus-var}
r_t = \mu_t + a_t
\end{equation}
where \(\mu_t\) represents the conditional mean and \(a_t\) is
modeled to capture the conditional variance.
\end{frame}

\begin{frame}[label={sec:org0ac4bbb}]{The mean equation}
\begin{align}
&\mu_t = E(r_t \mid F_{t-1}) = \sum_{i=1}^p \phi_i y_{t-i} - \sum_{i=1}^q \theta_i a_{t-i} \label{eq:mean-equation} \\
&y_t = r_t - \phi_0 - \sum_{i=1}^k \beta_i x_{it} \nonumber
\end{align}
\(F_{t-1}\) is the information set at time \(t-1\). 

\vspace{0.5cm}

If you combine these two equations, and let \(\mu_t = r_t - a_t\), you
will find that it is just an ARMA\((p, q)\) model with additional
regressors \(x_{it}\).
\end{frame}

\begin{frame}[label={sec:org664bf96}]{The variance equation}
Denote the conditional variance of \(r_t\) with \(\sigma^2_t\).
\begin{equation*}
\begin{split}
\sigma^2_t = \var(r_t \mid F_{t-1}) &= E\left( (r_t - E(r_t | F_{t-1}))^2 | F_{t-1} \right) \\
&= E\left( (r_t - \mu_t)^2 \mid F_{t-1} \right) \\
&= \var(a_t \mid F_{t-1})
\end{split}
\end{equation*}
\end{frame}

\begin{frame}[label={sec:org767b5ff}]{The variance equation (cont'd)}
\begin{itemize}
\item If we assume that \(E(a_t \mid F_{t-1}) = 0\), we can see that
\(\sigma^2_t = E(a^2_t \mid F_{t-1})\).

\item We can use the lagged value of \(a^2_t\) to represent the information
set \(F_{t-1}\)

\item The simplest model is a linear model, like the following
\[ \sigma^2_t = \alpha_0 + \alpha_1 a^2_{t-1} + \cdots + \alpha_m a^2_{t-m} \]
\end{itemize}
\end{frame}

\begin{frame}[label={sec:orgecbbe0e}]{The procedure of building a volatility model}
Building a volatility model for an asset return series consists of
four steps:

\begin{enumerate}
\item Specify a mean equation by testing for serial dependence in the
data and, if necessary, building an econometric model (e.g., an
ARMA model) for the return series to remove any linear dependence.

\item Use the squared residuals of the mean equation to test for ARCH
effects.

\item Specify a volatility model if ARCH effects are statistically
significant, and perform a joint estimation of the mean and
volatility equations.

\item Check the fitted model carefully and refine it if necessary.
\end{enumerate}
\end{frame}

\begin{frame}[label={sec:orgf3878c5}]{Testing for the presence of ARCH effect}
\begin{block}{The Ljung-Box test for the series of \(a^2_t\)}
Upon obtaining the residuals from the estimation
of an adequate mean equation, we can use the squared residuals
\{\(\hat{a}_t^2\)\} to test the existence of autocorrelation. 
\begin{itemize}
\item The Ljung-Box test is used to test the null hypothesis
\(H_0: \rho_1 = \cdots = \rho_m = 0\).
\item The \(Q(m)\) statistic is
calculated and compared with the critical value from \(\chi^2(m)\)
distribution at the desired significance level.
\item The rejection of the
null hypothesis implies that there is autoregressive conditional
heteroskedastic (ARCH) effect.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[label={sec:org9de398f}]{The LM test}
\begin{block}{An auxiliary regression}
We estimate a AR\((m)\) model regarding \{\(\hat{a}^2_t\)\}, that is,
\[ \hat{a}^2_t = \alpha_0 + \alpha_1 \hat{a}_{t-1}^2 + \cdots +
\alpha_m \hat{a}^2_{t-m} + e_t \]
\end{block}

\begin{block}{The LM test}
With this model, we test the joint hypothesis
\[H_0: \alpha_1 = \cdots = \alpha_m = 0 \]
\begin{itemize}
\item The LM statistic is \(NR^2\) where \(N\) is the sample size of this
regression and \(R^2\) is the coefficient of the determination of this
regression.
\item Given the null hypothesis is true, this statistic follows
a \(\chi^2(m)\) distribution.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[label={sec:orgad73270}]{The LM test (cont'd)}
Alternatively, we can use F statistic to test the joint
hypothesis. 
\begin{itemize}
\item Let \(SSR_0 = \sum_{t=m+1}^{T} (\hat{a}^2_{t} -
  \bar{\omega})^2\), where \(\bar{\omega} = (1/T) \sum_{t=1}^T
  \hat{a}^2_t\).
\item Let \(SSR_1 = \sum_{t=m+1}^T \hat{e}^2_t\) where \(\hat{e}_t\) is the
residuals from the regression.
\item The F statistic is
\[F = \frac{(SSR_0 - SSR_1)/m}{SSR_1/(T-2m-1)} \sim F(m, T-2m-1)\]
\item Rejecting the null hypothesis motivates us to model the possible
ARCH effect.
\end{itemize}
\end{frame}

\begin{frame}[label={sec:orgfa8cb6e}]{An example}
Go back to Figure \ref{fig:org96628fd}. Since the return series is
already stationary, we directly test the squared return series to
check the ARCH effect. 

\begin{itemize}
\item In the LM test of the ARCH effect, \(F = 53.62\) and the p value is
close to zero.
\item The Ljungâ€“Box statistics of the \(a^2_t\) series also
shows strong ARCH effects with \(Q(12) = 89.85\), the p value of which is
close to zero.
\item Therefore, we can confirm that the return series of
Intel stock has an ARCH effect, and next we need to model such an
effect.
\end{itemize}
\end{frame}

\section{The ARCH Model}
\label{sec:org27c248e}

\begin{frame}[label={sec:org872579d}]{The basic idea of an ARCH model}
Consider a series of shocks \{\(a_t\)\} in a return series \{\(r_t\)\}. The
basic idea of an Autoregressive Conditional Heteroskedasticity (ARCH)
model is

\begin{enumerate}
\item the shock \(a_t\) of the return series is serially uncorrelated but
dependent;

\item the dependence of \(a_t\) can be modeled through an autoregressive
process of \(a^2_t\).
\end{enumerate}
\end{frame}

\begin{frame}[label={sec:orga01f558}]{The ARCH(m) model}
An ARCH(m) model takes the following form
\begin{equation}
\label{eq:archm}
a_t = \sigma_t \epsilon_t,\; \sigma^2_t = \alpha_0 + \alpha_1 a^2_{t-1} + \cdots + \alpha_m a^2_{t-m}
\end{equation}
where \(\epsilon_t \sim i.i.d.(0, 1)\), \(\alpha_0 > 0\) and \(\alpha_i
\geq 0\) for \(i=1, \ldots, m\). 

\begin{itemize}
\item The assumption of \(\var(\epsilon_t)=1\) is to make the analysis
regarding the properties of the ARCH(m) model easy;
\item The assumption of \(\alpha_0 > 0\) and \(\alpha_i \geq 0\) is to ensure
the conditional variance of \(a_t\) is positive.
\item \(\alpha_1, \ldots, \alpha_m\) should also satisfy some regularity
conditions to ensure the unconditional variance of \(a_t\) is finite.
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org6d7c046}]{The Properties of an ARCH Model}
\begin{itemize}
\item Let's take an ARCH(1) model as an example to discuss the properties of
ARCH model.
\item The goal is to see how such a model can capture the basic idea mentioned
above and the stylized fact that highly volatile periods tend to be followed by
high volatility periods.
\end{itemize}

Assume an ARCH(1) model as follows
\begin{equation}
\label{eq:arch1}
a_t = \sigma_t \epsilon_t,\; \sigma^2_t = \alpha_0 + \alpha_1 a^2_{t-1},\; \epsilon_t \sim i.i.d.(0, 1)
\end{equation}
where \(a_0 > 0\) and \(a_1 \geq 0\). 
\end{frame}

\begin{frame}[label={sec:org706de2d}]{The unconditional mean and variance of \(a_t\)}
\begin{block}{The unconditional mean}
\begin{equation*}
E(a_t) & = E(\sigma_t \epsilon_t) = E(\sigma_t) E(\epsilon_t) = 0
\end{equation*}
The second equality is ensured because \(\sigma_t\) and \(\epsilon_t\) are
independent, and the third equality comes from the assumption of
\(E(\epsilon_t)=0\). 
\end{block}

\begin{block}{The unconditional variance}
\begin{equation*}
\begin{split}
\var(a_t) &= E(a^2_t) = E(\sigma^2_t \epsilon^2_t) \\
&= E(\alpha_0 + \alpha_1 a^2_{t-1}) \cdot 1 = \alpha_0 + \alpha_1\var(a_{t-1})
\end{split}
\end{equation*}

Assuming the unconditional mean of \(a_t\) is a constant(why?), we can
have 

\[\var(a_t) = \frac{\alpha_0}{1-\alpha_1} \]

Since the variance should be positive and finite, we must have \(0 \leq
\alpha_1 < 1\). 
\end{block}
\end{frame}

\begin{frame}[label={sec:org1bcf2ed}]{The unconditional covariance of \(a_t\)}
Since \(\epsilon_t\) and \(\epsilon_{t-i}\) for \(i \neq 0\) are independent, 

\begin{equation*}
\begin{split}
\cov(a_t, a_{t-i}) &= E(a_t a_{t-i}) = E(\sigma_t \epsilon_t \sigma_{t-i} \epsilon_{t-i}) \\
&= E(\sigma_t \sigma_{t-i}) E(\epsilon_t \epsilon_{t-i}) = 0
\end{split}
\end{equation*}

\begin{itemize}
\item What we get now?
\begin{itemize}
\item \(a_t\) has constant unconditional mean and variance,
\item \(a_t\) is serially uncorrelated.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[label={sec:orga952c3f}]{The kurtosis of \(a_t\)}
\begin{itemize}
\item Assume that \(\epsilon \sim N(0, 1)\), implying that \(E(\epsilon^4_t) =
  3\). Thus, we have
\begin{equation*}
\begin{split}
E(a^4_t) &= E(\sigma^4_t \epsilon_t^4) = E(\sigma^4_t) E(\epsilon^4_t) = 3 E(\sigma^4_t) \\
&= 3\left(\alpha^2_0 + 2\alpha_0\alpha_1 E(a^2_{t-1}) + \alpha^2_1 E(a^4_{t-1}) \right)
\end{split}
\end{equation*}

\item Assume that \(a_t\) is fourth-order stationary so that we can define
\(m_4 = E(a^4_t) = E(a^4_{t-1})\). Then, using the fact that \(E(a^2_t) =
  \alpha_0 /(1-\alpha_1)\), we can solve \(m_4\) from the
above equation.
\[m_4 = \frac{3\alpha^2_0(1+\alpha_1)}{(1-\alpha_1)(1-3\alpha^2_1)}
  \]
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org9f8bd92}]{The kurtosis of \(a_t\) (cont'd)}
This result regarding \(m_4\) has two important implications: 
\begin{enumerate}
\item Since the fourth moment of \(a_t\) is positive, we see that \(\alpha_1\) must
also satisfy the condition \(1-3\alpha_1^2 > 0\), that is, \(0 \leq
   \alpha^2_1 < \frac{1}{3}\).
\item The kurtosis of \(a_t\) is
\[\text{kurtosis} = \frac{E(a^4_t)}{E(a^2_t)^2} =
   \frac{3(1-\alpha^2_1)}{1-3\alpha_1^2}  > 3\]

Thus, the the excess kurtosis of \(a_t\) is positive and the tail
distribution of \(a_t\) is heavier than that of a normal
distribution.
\end{enumerate}
\end{frame}

\begin{frame}[label={sec:orgbeda375}]{The conditional mean}
\begin{itemize}
\item Let's write \(E_{t-1}(a_t)\) to represent the conditional mean given the
information set \(F_{t-1}\), i.e., \(E(E(a_t \mid F_{t-1}))\).

\item Since \(\epsilon_t\) is i.i.d, we have \(E_{t-1}(\epsilon_t) =
  E(\epsilon_t) = 0\). Thus, 

\begin{equation*}
\begin{split}
E_{t-1}(a_t) &= E_{t-1}(\sigma_t \epsilon_t) = E_{t-1}\left((\alpha_0 + \alpha_1 a^2_{t-1})^{1/2} \epsilon_t\right) \\
&= (\alpha_0 + \alpha_1 a^2_{t-1})^{1/2} E_{t-1}(\epsilon_t) = 0
\end{split}
\end{equation*}
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org1246b88}]{The conditional variance}
\begin{itemize}
\item The conditional variance of \(a_t\) is 
\begin{equation*}
\begin{split}
\var_{t-1}(a_t) &= E_{t-1}(a^2_t) = E_{t-1} \left( \sigma^2_t \epsilon_t^2 \right) \\
&= E_{t-1}\left((\alpha_0 + \alpha_1 a^2_{t-1}) \epsilon^2_t \right) \\ 
&= E_{t-1}(\alpha_0 + \alpha_1 a^2_{t-1}) E_{t-1}(\epsilon^2_t) \\
&= (\alpha_0 + \alpha_1 a^2_{t-1}) E(\epsilon^2_t) \\
&= \alpha_0 + \alpha_1 a^2_{t-1} = \sigma^2_t
\end{split}
\end{equation*}

\item How does the conditional variance capture the stylized fact?
\end{itemize}
\end{frame}

\section{Estimation and Forecasting}
\label{sec:org499dc0e}

\begin{frame}[label={sec:orga538a75}]{Order determination}
\begin{itemize}
\item Before estimating an ARCH(m) model, we need to determine the order
\(m\).

\vspace{0.2cm}

\item The basic idea is that we treat an ARCH(m) model as an AR process
of \{\(a^2_t\)\}, and apply the partial autocorrelation function (PACF) to
determine \(m\).
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org25793a0}]{Why using the PACF?}
We justify the use of the PACF of \{\(a^2_t\)\} to determine \(m\) through
two perspectives.

\vspace{0.2cm}

\begin{enumerate}
\item We can consider \(a^2_t\) as an unbiased estimator of \(\sigma^2_t\)
given the sample data because \(E_{t-1}(a^2_t) =
   \sigma^2_t\). Therefore, we use \(a^2_t\) as an approximate to
\(\sigma^2_t\).

\vspace{0.2cm}

\item We can define \(\eta_t = a^2_t - \sigma^2_t\). It can be shown that
\begin{itemize}
\item \(E(\eta_t) = 0\) and \(E(\eta_t \eta_{t-s})=0\) for \(s > 0\).
\item But \(\eta_t\) is not i.i.d. because \(a^2_t\) is dependent.
\end{itemize}
\vspace{0.2cm}

So an ARCH(m) model is essentially an AR(m) model, except
that \(\eta_t\) is not i.i.d. That is,
\[ a^2_t = \alpha_0 + \alpha_1 a^2_{t-1} + \cdots + \alpha_m
   a^2_{t-m} + \eta_t \]
\end{enumerate}
\end{frame}
\end{document}