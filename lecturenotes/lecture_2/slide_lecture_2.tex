% Created 2017-05-13 Sat 17:16
% Intended LaTeX compiler: pdflatex
\documentclass[presentation,10pt]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\newtheorem{mydef}{Definition}
\newtheorem{mythm}{Theorem}
\newcommand{\dx}{\mathrm{d}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\corr}{\mathrm{corr}}
\newcommand{\pr}{\mathrm{Pr}}
\newcommand{\rarrowd}[1]{\xrightarrow{\text{ \textit #1 }}}
\DeclareMathOperator*{\plim}{plim}
\newcommand{\plimn}{\plim_{n \rightarrow \infty}}
\usepackage{booktabs}
\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}
\def\mathbi#1{\textbf{\em #1}}
\setlength{\parskip}{1em}
\newcommand{\undersetdisp}[2]{\underset{\displaystyle #1}{#2}}
\usetheme{CambridgeUS}
\usecolortheme{beaver}
\author{Zheng Tian}
\date{}
\title{Lecture 2: The ARCH Model}
\hypersetup{
 pdfauthor={Zheng Tian},
 pdftitle={Lecture 2: The ARCH Model},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 25.1.1 (Org mode 9.0.3)}, 
 pdflang={English}}
\begin{document}

\maketitle
\begin{frame}{Outline}
\tableofcontents
\end{frame}


\section{The Volatility of Asset Returns}
\label{sec:orgf84a6a6}

\begin{frame}[label={sec:org2ff9f88}]{Volatility is measured with the conditional variance}
\begin{itemize}
\item Volatility here refers to the \alert{conditional variance} of a time series.

\item For a return series \{\(r_t\)\}, we are now interested in
\[\sigma^2_t = \var(r_t \mid F_{t-1})\]
where \(F_{t-1}\) is the information set at time \(t-1\).
\end{itemize}
\end{frame}

\begin{frame}[label={sec:orgcb9521c}]{Characteristics of volatility (1)}
\begin{enumerate}
\item \alert{There exist volatility clusters.} That is, volatility may be high
for certain time periods and low for other periods.
\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{img/nyse_us100.png}
\caption{\label{fig:orgb4e5e0b}
Percentage Change in the NYSE U.S.100 stock price index}
\end{figure}
\end{enumerate}
\end{frame}

\begin{frame}[label={sec:orgf0405a1}]{Characteristics of volatility (2)}
\begin{enumerate}
\item Volatility evolves over time in a continuous manner. That is,
volatility jumps are rare.
\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{img/readgdp.png}
\caption{\label{fig:org48745df}
Annualized Growth Rate of Real GDP}
\end{figure}
\end{enumerate}
\end{frame}

\begin{frame}[label={sec:org74ad74e}]{Characteristics of volatility (3)}
\begin{enumerate}
\item Volatility does not diverge to infinity. That is, volatility varies
within some fixed range. Statistically speaking, this means that
volatility is often stationary.
\end{enumerate}

\vspace{0.5cm}

\begin{enumerate}
\item Volatility seems to react differently to a big price increase or a
big price drop, referred to as the leverage effect.
\end{enumerate}
\end{frame}

\section{The Structure of a Volatility Model}
\label{sec:orgd1eed9e}

\begin{frame}[label={sec:org0a903d2}]{The basic idea of building a volatility model}
Consider the log return series \{\(r_t\)\}. The basic idea of a volatility
model is
\begin{itemize}
\item \{\(r_t\)\} may appear to be either serially uncorrelated or
serially correlated with a minor order.
\item However, \{\(r_t\)\} is a dependent series and the dependence arises
from its conditional variance.
\end{itemize}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth,height=0.4\textheight]{img/intel.png}
\caption{\label{fig:org0b29ba9}
Time plot of monthly log returns of Intel stock from January 1973 to December 2008}
\end{figure}
\end{frame}

\begin{frame}[label={sec:orgf1597f6}]{The sample ACF of \{\(r_t\)\} and \{\(r^2_t\)\}}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth,height=0.5\textheight]{img/acf_intel.png}
\caption{\label{fig:org65a2dba}
Sample ACF and PACF of various functions of monthly log stock returns of Intel Corporation from January 1973 to December 2008: (a) ACF of the log returns, (b) ACF of the squared log returns, (c) ACF of the absolute log returns, and (d) PACF of the squared log returns.}
\end{figure}
\end{frame}

\begin{frame}[label={sec:org5b9bcc3}]{Decompose \(r_t\) into the mean and variance equations}
To capture the dependence in a time series through its second moment
but not the mean, we model the mean process and the variance process
separately.

For a return series \{\(r_t\)\}, we can model it as
\begin{equation}
\label{eq:mean-plus-var}
r_t = \mu_t + a_t
\end{equation}
where \(\mu_t\) represents the conditional mean and \(a_t\) is
modeled to capture the conditional variance.
\end{frame}

\begin{frame}[label={sec:org864895f}]{The mean equation}
\begin{align}
&\mu_t = E(r_t \mid F_{t-1}) = \sum_{i=1}^p \phi_i y_{t-i} - \sum_{i=1}^q \theta_i a_{t-i} \label{eq:mean-equation} \\
&y_t = r_t - \phi_0 - \sum_{i=1}^k \beta_i x_{it} \nonumber
\end{align}
\(F_{t-1}\) is the information set at time \(t-1\).

\vspace{0.5cm}

If you combine these two equations, and let \(\mu_t = r_t - a_t\), you
will find that it is just an ARMA\((p, q)\) model with additional
regressors \(x_{it}\).
\end{frame}

\begin{frame}[label={sec:orgecd8943}]{The variance equation}
Denote the conditional variance of \(r_t\) with \(\sigma^2_t\).
\begin{equation*}
\begin{split}
\sigma^2_t = \var(r_t \mid F_{t-1}) &= E\left( (r_t - E(r_t | F_{t-1}))^2 | F_{t-1} \right) \\
&= E\left( (r_t - \mu_t)^2 \mid F_{t-1} \right) \\
&= \var(a_t \mid F_{t-1})
\end{split}
\end{equation*}
\end{frame}

\begin{frame}[label={sec:orga1e099f}]{The variance equation (cont'd)}
\begin{itemize}
\item If we assume that \(E(a_t \mid F_{t-1}) = 0\), we can see that
\(\sigma^2_t = E(a^2_t \mid F_{t-1})\).

\item We can use the lagged value of \(a^2_t\) to represent the information
set \(F_{t-1}\)

\item The simplest model is a linear model, like the following
\[ \sigma^2_t = \alpha_0 + \alpha_1 a^2_{t-1} + \cdots + \alpha_m a^2_{t-m} \]
\end{itemize}
\end{frame}

\begin{frame}[label={sec:orgeb24a51}]{The procedure of building a volatility model}
Building a volatility model for an asset return series consists of
four steps:

\begin{enumerate}
\item Specify a mean equation by testing for serial dependence in the
data and, if necessary, building an econometric model (e.g., an
ARMA model) for the return series to remove any linear dependence.

\item Use the squared residuals of the mean equation to test for ARCH
effects.

\item Specify a volatility model if ARCH effects are statistically
significant, and perform a joint estimation of the mean and
volatility equations.

\item Check the fitted model carefully and refine it if necessary.
\end{enumerate}
\end{frame}

\begin{frame}[label={sec:org0b3a569}]{Testing for the presence of ARCH effect}
\begin{block}{The Ljung-Box test for the series of \(a^2_t\)}
Upon obtaining the residuals from the estimation
of an adequate mean equation, we can use the squared residuals
\{\(\hat{a}_t^2\)\} to test the existence of autocorrelation.
\begin{itemize}
\item The Ljung-Box test is used to test the null hypothesis
\(H_0: \rho_1 = \cdots = \rho_m = 0\).
\item The \(Q(m)\) statistic is
calculated and compared with the critical value from \(\chi^2(m)\)
distribution at the desired significance level.
\item The rejection of the
null hypothesis implies that there is autoregressive conditional
heteroskedastic (ARCH) effect.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[label={sec:org639776e}]{The LM test}
\begin{block}{An auxiliary regression}
We estimate a AR\((m)\) model regarding \{\(\hat{a}^2_t\)\}, that is,
\[ \hat{a}^2_t = \alpha_0 + \alpha_1 \hat{a}_{t-1}^2 + \cdots +
\alpha_m \hat{a}^2_{t-m} + e_t \]
\end{block}

\begin{block}{The LM test}
With this model, we test the joint hypothesis
\[H_0: \alpha_1 = \cdots = \alpha_m = 0 \]
\begin{itemize}
\item The LM statistic is \(NR^2\) where \(N\) is the sample size of this
regression and \(R^2\) is the coefficient of the determination of this
regression.
\item Given the null hypothesis is true, this statistic follows
a \(\chi^2(m)\) distribution.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[label={sec:org538519a}]{The LM test (cont'd)}
Alternatively, we can use F statistic to test the joint
hypothesis.
\begin{itemize}
\item Let \(SSR_0 = \sum_{t=m+1}^{T} (\hat{a}^2_{t} -
  \bar{\omega})^2\), where \(\bar{\omega} = (1/T) \sum_{t=1}^T
  \hat{a}^2_t\).
\item Let \(SSR_1 = \sum_{t=m+1}^T \hat{e}^2_t\) where \(\hat{e}_t\) is the
residuals from the regression.
\item The F statistic is
\[F = \frac{(SSR_0 - SSR_1)/m}{SSR_1/(T-2m-1)} \sim F(m, T-2m-1)\]
\item Rejecting the null hypothesis motivates us to model the possible
ARCH effect.
\end{itemize}
\end{frame}

\begin{frame}[label={sec:orgfa55e75}]{An example}
Go back to Figure \ref{fig:org65a2dba}. Since the return series is
already stationary, we directly test the squared return series to
check the ARCH effect.

\begin{itemize}
\item In the LM test of the ARCH effect, \(F = 53.62\) and the p value is
close to zero.
\item The Ljungâ€“Box statistics of the \(a^2_t\) series also
shows strong ARCH effects with \(Q(12) = 89.85\), the p value of which is
close to zero.
\item Therefore, we can confirm that the return series of
Intel stock has an ARCH effect, and next we need to model such an
effect.
\end{itemize}
\end{frame}

\section{The ARCH Model}
\label{sec:org8975407}

\begin{frame}[label={sec:orgdbe93ae}]{The basic idea of an ARCH model}
Consider a series of shocks \{\(a_t\)\} in a return series \{\(r_t\)\}. The
basic idea of an Autoregressive Conditional Heteroskedasticity (ARCH)
model is

\begin{enumerate}
\item the shock \(a_t\) of the return series is serially uncorrelated but
dependent;

\item the dependence of \(a_t\) can be modeled through an autoregressive
process of \(a^2_t\).
\end{enumerate}
\end{frame}

\begin{frame}[label={sec:orgb216acd}]{The ARCH(m) model}
An ARCH(m) model takes the following form
\begin{equation}
\label{eq:archm}
a_t = \sigma_t \epsilon_t,\; \sigma^2_t = \alpha_0 + \alpha_1 a^2_{t-1} + \cdots + \alpha_m a^2_{t-m}
\end{equation}
where \(\epsilon_t \sim i.i.d.(0, 1)\), \(\alpha_0 > 0\) and \(\alpha_i
\geq 0\) for \(i=1, \ldots, m\).

\begin{itemize}
\item The assumption of \(\var(\epsilon_t)=1\) is to make the analysis
regarding the properties of the ARCH(m) model easy;
\item The assumption of \(\alpha_0 > 0\) and \(\alpha_i \geq 0\) is to ensure
the conditional variance of \(a_t\) is positive.
\item \(\alpha_1, \ldots, \alpha_m\) should also satisfy some regularity
conditions to ensure the unconditional variance of \(a_t\) is finite.
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org9e4b82d}]{The Properties of an ARCH Model}
\begin{itemize}
\item Let's take an ARCH(1) model as an example to discuss the properties of
ARCH model.
\item The goal is to see how such a model can capture the basic idea mentioned
above and the stylized fact that highly volatile periods tend to be followed by
high volatility periods.
\end{itemize}

Assume an ARCH(1) model as follows
\begin{equation}
\label{eq:arch1}
a_t = \sigma_t \epsilon_t,\; \sigma^2_t = \alpha_0 + \alpha_1 a^2_{t-1},\; \epsilon_t \sim i.i.d.(0, 1)
\end{equation}
where \(a_0 > 0\) and \(a_1 \geq 0\).
\end{frame}

\begin{frame}[label={sec:org4e80969}]{The unconditional mean and variance of \(a_t\)}
\begin{block}{The unconditional mean}
\begin{equation*}
E(a_t) & = E(\sigma_t \epsilon_t) = E(\sigma_t) E(\epsilon_t) = 0
\end{equation*}
The second equality is ensured because \(\sigma_t\) and \(\epsilon_t\) are
independent, and the third equality comes from the assumption of
\(E(\epsilon_t)=0\).
\end{block}

\begin{block}{The unconditional variance}
\begin{equation*}
\begin{split}
\var(a_t) &= E(a^2_t) = E(\sigma^2_t \epsilon^2_t) \\
&= E(\alpha_0 + \alpha_1 a^2_{t-1}) \cdot 1 = \alpha_0 + \alpha_1\var(a_{t-1})
\end{split}
\end{equation*}

Assuming the unconditional mean of \(a_t\) is a constant(why?), we can
have

\[\var(a_t) = \frac{\alpha_0}{1-\alpha_1} \]

Since the variance should be positive and finite, we must have \(0 \leq
\alpha_1 < 1\).
\end{block}
\end{frame}

\begin{frame}[label={sec:org0b496b2}]{The unconditional covariance of \(a_t\)}
Since \(\epsilon_t\) and \(\epsilon_{t-i}\) for \(i \neq 0\) are independent,

\begin{equation*}
\begin{split}
\cov(a_t, a_{t-i}) &= E(a_t a_{t-i}) = E(\sigma_t \epsilon_t \sigma_{t-i} \epsilon_{t-i}) \\
&= E(\sigma_t \sigma_{t-i}) E(\epsilon_t \epsilon_{t-i}) = 0
\end{split}
\end{equation*}

\begin{itemize}
\item What we get now?
\begin{itemize}
\item \(a_t\) has constant unconditional mean and variance,
\item \(a_t\) is serially uncorrelated.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org498618f}]{The kurtosis of \(a_t\)}
\begin{itemize}
\item Assume that \(\epsilon \sim N(0, 1)\), implying that \(E(\epsilon^4_t) =
  3\). Thus, we have
\begin{equation*}
\begin{split}
E(a^4_t) &= E(\sigma^4_t \epsilon_t^4) = E(\sigma^4_t) E(\epsilon^4_t) = 3 E(\sigma^4_t) \\
&= 3\left(\alpha^2_0 + 2\alpha_0\alpha_1 E(a^2_{t-1}) + \alpha^2_1 E(a^4_{t-1}) \right)
\end{split}
\end{equation*}

\item Assume that \(a_t\) is fourth-order stationary so that we can define
\(m_4 = E(a^4_t) = E(a^4_{t-1})\). Then, using the fact that \(E(a^2_t) =
  \alpha_0 /(1-\alpha_1)\), we can solve \(m_4\) from the
above equation.
\[m_4 = \frac{3\alpha^2_0(1+\alpha_1)}{(1-\alpha_1)(1-3\alpha^2_1)}
  \]
\end{itemize}
\end{frame}

\begin{frame}[label={sec:orgdf71431}]{The kurtosis of \(a_t\) (cont'd)}
This result regarding \(m_4\) has two important implications:
\begin{enumerate}
\item Since the fourth moment of \(a_t\) is positive, we see that \(\alpha_1\) must
also satisfy the condition \(1-3\alpha_1^2 > 0\), that is, \(0 \leq
   \alpha^2_1 < \frac{1}{3}\).
\item The kurtosis of \(a_t\) is
\[\text{kurtosis} = \frac{E(a^4_t)}{E(a^2_t)^2} =
   \frac{3(1-\alpha^2_1)}{1-3\alpha_1^2}  > 3\]

Thus, the the excess kurtosis of \(a_t\) is positive and the tail
distribution of \(a_t\) is heavier than that of a normal
distribution.
\end{enumerate}
\end{frame}

\begin{frame}[label={sec:orgb5eaced}]{The conditional mean}
\begin{itemize}
\item Let's write \(E_{t-1}(a_t)\) to represent the conditional mean given the
information set \(F_{t-1}\), i.e., \(E(E(a_t \mid F_{t-1}))\).

\item Since \(\epsilon_t\) is i.i.d, we have \(E_{t-1}(\epsilon_t) =
  E(\epsilon_t) = 0\). Thus,

\begin{equation*}
\begin{split}
E_{t-1}(a_t) &= E_{t-1}(\sigma_t \epsilon_t) = E_{t-1}\left((\alpha_0 + \alpha_1 a^2_{t-1})^{1/2} \epsilon_t\right) \\
&= (\alpha_0 + \alpha_1 a^2_{t-1})^{1/2} E_{t-1}(\epsilon_t) = 0
\end{split}
\end{equation*}
\end{itemize}
\end{frame}

\begin{frame}[label={sec:orgafb0d33}]{The conditional variance}
\begin{itemize}
\item The conditional variance of \(a_t\) is
\begin{equation*}
\begin{split}
\var_{t-1}(a_t) &= E_{t-1}(a^2_t) = E_{t-1} \left( \sigma^2_t \epsilon_t^2 \right) \\
&= E_{t-1}\left((\alpha_0 + \alpha_1 a^2_{t-1}) \epsilon^2_t \right) \\
&= E_{t-1}(\alpha_0 + \alpha_1 a^2_{t-1}) E_{t-1}(\epsilon^2_t) \\
&= (\alpha_0 + \alpha_1 a^2_{t-1}) E(\epsilon^2_t) \\
&= \alpha_0 + \alpha_1 a^2_{t-1} = \sigma^2_t
\end{split}
\end{equation*}

\item How does the conditional variance capture the stylized fact?
\end{itemize}
\end{frame}

\section{Maximum Likelihood Estimation}
\label{sec:orgb6096c6}

\begin{frame}[label={sec:org09222d7}]{The definition of the likelihood function}
The likelihood function is the joint density function \(f(\mathbf{y} |
\boldsymbol{\theta})\) when we consider it as a function of the
parameters \(\boldsymbol{\theta}\) given a set of data \(\mathbf{y}\).

\begin{itemize}
\item \(\mathbf{y} = (y_1, \ldots, y_T)\) represents the observations,
which are assumed to be identically independently distributed.
\item \(\boldsymbol{\theta} = (\theta_1, \ldots, \theta_k)\) represents all
parameters in the model (i.e., data generating process) that
generates \(\mathbf{y}\).
\item For each observation \(y_t\), its (marginal) PDF is \(f(y_t |
  \boldsymbol{\theta})\).
\end{itemize}
\end{frame}

\begin{frame}[label={sec:orgc11df8b}]{The likelihood function when \(y_t\) is independent}
\begin{block}{The likelihood function}
\begin{equation}
\label{eq:general-likelihood}
L(\boldsymbol{\theta} | \mathbf{y}) = \prod_{t=1}^T f(y_t | \boldsymbol{\theta})
\end{equation}
\end{block}

\begin{block}{The log-likelihood function}
\begin{equation}
\label{eq:general-logL}
\ell(\boldsymbol{\theta} | \mathbf{y}) = \sum_{t=1}^T \ell_t(\boldsymbol{\theta} | y_t)
\end{equation}
where \(\ell_t(\boldsymbol{\theta} | y_t) = \ln\left(f(y_t |
\boldsymbol{\theta})\right)\) is the \alert{contribution} to the
loglikelihood function.
\end{block}
\end{frame}

\begin{frame}[label={sec:org4217cfa}]{The joint density when \(y_t\) is dependent}
If \(y_1, y_2, \ldots, y_T\) are dependent, their joint density can be
written as
\[ f(y_1, y_2, \ldots, y_T) = f(y_1) f(y_2 | y_1) \cdots f(y_T | y_1,
\ldots, y_{T-1}) \]
or conveniently denoted as
\[f(\mathbf{y}^T) = \prod_{t=1}^T f(y_t | \mathbf{y}^{t-1}) \]
\end{frame}

\begin{frame}[label={sec:org0e73dd3}]{The likelihood and log-likelihood functions when \(y_t\) is dependent}
When \(\mathbf{y}\) is also dependent on \(\boldsymbol{\theta}\), the
likelihood function is then
\begin{equation}
\label{eq:depend-likelihood}
L(\boldsymbol{\theta} | \mathbf{y}^T) = \prod_{t=1}^T f(y_t | \mathbf{y}^{t-1}, \boldsymbol{\theta})
\end{equation}
And the log-likelihood function is
\begin{equation}
\label{eq:depend-logL}
\ell(\boldsymbol{\theta} | \mathbf{y}^T) = \sum_{t=1}^T \ell_t(\boldsymbol{\theta} | \mathbf{y}^{t})
\end{equation}
\end{frame}

\begin{frame}[label={sec:org5bcd707}]{The maximum likelihood (ML) estimator}
\begin{itemize}
\item The ML estimator maximizes the log-likelihood function over the
parameter space in which \(\boldsymbol{\theta}\) lies in.

\begin{equation}
\operatorname*{max}_{\{\theta \in \Theta\}}\: \ell(\boldsymbol{\theta} | \mathbf{y}^T) = \sum_{t=1}^T \ell_t(\boldsymbol{\theta} | \mathbf{y}^t)
\end{equation}

\item The ML estimator is usually obtained by computational methods, like
the Newton or quasi-Newton method.
\end{itemize}
\end{frame}

\section{Estimation of an ARCH(m) model}
\label{sec:org8a3c952}

\begin{frame}[label={sec:org08c310f}]{The assumption of the distribution of \(\epsilon_t\)}
Consider an ARCH(m) model

\begin{equation*}
a_t = \sigma_t \epsilon_t,\; \sigma^2_t = \alpha_0 + \alpha_1 a^2_{t-1} + \cdots + \alpha_m a^2_{t-m}
\end{equation*}

\begin{itemize}
\item Assume that \(\epsilon_t \sim N(0, 1)\) and \(\epsilon_t\) is i.i.d.

\item How can we get the likelihood function of an ARCH(m) model based on
this assumption?
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org3080be6}]{The distribution of \(a_t\)}
\begin{block}{The conditional distribution of \(a_t\)}
\[ \epsilon_t \sim N(0, 1) \Rightarrow a_t \mid F_{t-1} \sim N(0,
\sigma^2_t) \] 
where \(\sigma^2_t\) is given by the ARCH(m) model. The
conditional PDF of each \(a_t\) for \(t=1, \ldots, T\) is
\end{block}

\begin{block}{The conditional PDF of \(a_t\)}
\[
f(a_t | F_{t-1}) = \frac{1}{\sqrt{2 \pi \sigma^2_t}} \exp
\left(-\frac{a^2_t}{2\sigma^2_t}\right)
\] 
where \(F_{t-1}\) is the information set represented by \((a_1, \ldots,
a_{t-1})\).
\end{block}
\end{frame}

\begin{frame}[label={sec:org487695a}]{The joint density for \(a_1, \ldots, a_T\) given the parameter vector \(\mathbf{\alpha}\)}
\begin{itemize}
\item For convenience, we suppress \(\boldsymbol{\alpha} = (\alpha_0,
  \alpha_1, \ldots, \alpha_m)\) in the
conditional joint density \(f(a_1, \ldots, a_T \mid
  \boldsymbol{\alpha})\).

\item Since \(a_1, \ldots, a_T\) are not independent, we have the joint
density as
\begin{align}
f(a_1, a_2, \ldots, a_T) &= f(a_T | F_{T-1}) f(a_{T-1} | F_{T-2}) \cdots f(a_{m+1} | F_m) f(a_1, \ldots, a_m) \nonumber \\
&= \prod_{t=m+1}^T \frac{1}{\sqrt{2 \pi \sigma^2_t}} \exp \left(-\frac{a^2_t}{2\sigma^2_t}\right) \times f(a_1, \ldots, a_m)
\end{align}
\end{itemize}
\end{frame}

\begin{frame}[label={sec:orgd8126c9}]{The conditional likelihood function}
Dropping \(f(a_1, \ldots, a_m)\) because the exact form of it is often
complicated, we get the conditional likelihood function
\begin{equation}
\label{eq:arch-likelihood}
L(\boldsymbol{\alpha} |a_1, \ldots, a_T) = \prod_{t=m+1}^T \frac{1}{\sqrt{2 \pi \sigma^2_t}} \exp \left(-\frac{a^2_t}{2\sigma^2_t}\right)
\end{equation}

\begin{itemize}
\item \(\boldsymbol{\alpha}\) enters the likelihood function through 
\[\sigma^2_t = \alpha_0 + \alpha_1 a^2_{t-1} + \cdots + \alpha_m
  a^2_{t-m} \]

\item Maximizing the likelihood function results in the maximum likelihood
estimator under the normality assumption.
\end{itemize}
\end{frame}

\begin{frame}[label={sec:orgfcb8ece}]{The conditional log-likelihood function}
Taking logarithm of the likelihood function yields the log-likelihood
function
\begin{equation}
\label{eq:arch-logL}
\ell(\boldsymbol{\alpha} | a_1, \ldots, a_T) = 
\sum_{t=m+1}^T \left[ -\frac{1}{2} \ln(2\pi) - \frac{1}{2} \ln(\sigma^2_t) - \frac{1}{2} \frac{a^2_t}{\sigma^2_t}  \right]
\end{equation}

The log-likelihood function can be simplified as 
\[
\ell(\boldsymbol{\alpha} | a_1, \ldots, a_T) = -\frac{1}{2}
\sum_{t=m+1}^T \left[\ln(\sigma^2_t) + \frac{a^2_t}{\sigma^2_t}  \right]
\]
\end{frame}

\begin{frame}[label={sec:orgb2898d3}]{Order determination}
\begin{itemize}
\item Before estimating an ARCH(m) model, we need to determine the order
\(m\).

\vspace{0.2cm}

\item The basic idea is that we treat an ARCH(m) model as an AR process
of \{\(a^2_t\)\}, and apply the partial autocorrelation function (PACF) to
determine \(m\).
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org9b7ed62}]{Why using the PACF?}
We justify the use of the PACF of \{\(a^2_t\)\} to determine \(m\) through
two perspectives.

\vspace{0.2cm}

\begin{enumerate}
\item We can consider \(a^2_t\) as an unbiased estimator of \(\sigma^2_t\)
given the sample data because \(E_{t-1}(a^2_t) =
   \sigma^2_t\). Therefore, we use \(a^2_t\) as an approximate to
\(\sigma^2_t\).

\vspace{0.2cm}

\item We can define \(\eta_t = a^2_t - \sigma^2_t\). It can be shown that
\begin{itemize}
\item \(E(\eta_t) = 0\) and \(E(\eta_t \eta_{t-s})=0\) for \(s > 0\).
\item But \(\eta_t\) is not i.i.d. because \(a^2_t\) is dependent.
\end{itemize}
\vspace{0.2cm}

So an ARCH(m) model is essentially an AR(m) model, except
that \(\eta_t\) is not i.i.d. That is,
\[ a^2_t = \alpha_0 + \alpha_1 a^2_{t-1} + \cdots + \alpha_m
   a^2_{t-m} + \eta_t \]
\end{enumerate}
\end{frame}

\begin{frame}[label={sec:org2ed1274}]{Alternative distribution assumptions of \(\epsilon_t\)}
\begin{itemize}
\item \(\epsilon_t\) has a Student-t distribution: capture the heavy-tail
of volatility.

\item \(\epsilon_t\) has a skew-Student-t distribution: capture the skewness
of asset return.

\item \(\epsilon_t\) has a general error distribution: enclose the normal
distribution and heavy-tail distributions.
\end{itemize}
\end{frame}

\section{Model checking and forecasting}
\label{sec:org9542a7c}

\begin{frame}[label={sec:orgef98b31}]{Model checking}
\begin{block}{The standardized residuals}
Compute the standardized residuals
\[ \tilde{a}_t = \frac{\hat{a}_t}{\hat{\sigma}_t} \]
which should mimic the behavior of \(\epsilon_t\). 
\end{block}

\begin{block}{Check the mean equation}
\begin{itemize}
\item Use the Ljung-Box statistic for \{\(\tilde{a}_t\)\}.
\end{itemize}
\end{block}

\begin{block}{Check the volatility equation}
\begin{itemize}
\item Use the Ljung-Box statistic for \{\(\tilde{a}_t\)\}.
\item Use the QQ plot or the Shapiro-Wilk test for normality assumption.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[label={sec:org39e7f0e}]{Forecasting}
\begin{block}{1-step-ahead forecast}
\[\sigma^2_h(1) = \alpha_0 + \alpha_1 a^2_h + \cdots + \alpha_m
a^2_{h+1-m} \]
\end{block}

\begin{block}{2-step-ahead forecast}
\[ \sigma^2_h(2) = \alpha_0 + \alpha_1 \sigma^2_h(1) + \alpha_2
a^2_h + \cdots + \alpha_m a^2_{h+2-m}  \]
\end{block}

\begin{block}{\(\ell\)-step-ahead forecast}
\[ \sigma^2_h(\ell) = \alpha_0 + \sum_{i=1}^m \alpha_i
\sigma^2_h(\ell-i) \]
\end{block}
\end{frame}

\section{Applications of ARCH Models}
\label{sec:org6055b95}

\begin{frame}[label={sec:org97765af}]{An ARCH model for the monthly log returns of Intel stock}
\begin{block}{The proposed model}
\[ r_t = \mu + \alpha_t, a_t = \sigma_t \epsilon_t, \sigma^2_t =
\alpha_0 + \alpha_1 a^2_{t-1} + \alpha_2 a^2_{t-2} + \alpha_3 a^2_{t-3} \]
\end{block}

\begin{block}{The estimated model}
\[ r_t = \underset{(0.0057)}{0.0122} + a_t, 
\sigma^2_t = \underset{(0.0010)}{0.0106} + 
             \underset{(0.0757)}{0.2131}a^2_{t-1} +
             \underset{(0.0480)}{0.0770}a^2_{t-2} + 
             \underset{(0.0688)}{0.0599}a^2_{t-3}
\]

Since \(\alpha_2\) and \(\alpha_3\) are statistically insignificant, we
drop the last two terms and re-estimate the model.
\[
r_t = \underset{(0.0053)}{0.0126} + a_t, 
\sigma^2_t = \underset{(0.0010)}{0.0111} + \underset{(0.0761)}{0.3560}a^2_{t-1}
\]
\end{block}
\end{frame}

\begin{frame}[label={sec:org2e81da3}]{Model checking}
\begin{block}{The Ljung-Box test}
\begin{itemize}
\item \(Q(10)\) for \{\(\tilde{a}_t\)\} is 12.64, p-value = 0.24 \(\Rightarrow\) no
autocorrelation.
\item \(Q(10)\) for \{\(\tilde{a}^2_t\)\} is 14.75, p-value = 0.14 \(\Rightarrow\)
no autocorrelation.
\end{itemize}
\end{block}

\begin{block}{The sample ACF}
\begin{center}
\includegraphics[width=0.9\textwidth,height=0.6\textheight]{img/intel_arch.png}
\end{center}
\end{block}
\end{frame}
\end{document}